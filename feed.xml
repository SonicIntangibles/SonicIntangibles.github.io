<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sonicintangibles.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sonicintangibles.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T13:42:24+00:00</updated><id>https://sonicintangibles.github.io/feed.xml</id><title type="html">Sonic Intangibles</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Paper of The Fortnight</title><link href="https://sonicintangibles.github.io/stories/2025/POTF/" rel="alternate" type="text/html" title="Paper of The Fortnight"/><published>2025-05-11T00:00:00+00:00</published><updated>2025-05-11T00:00:00+00:00</updated><id>https://sonicintangibles.github.io/stories/2025/POTF</id><content type="html" xml:base="https://sonicintangibles.github.io/stories/2025/POTF/"><![CDATA[<p>Each fortnight a member of the Sonic Intangibles team select a paper to read and discuss. Our reading list is below:</p> <ol> <li> <p>Akkerman, S.F. and Bakker, A., 2011. Boundary crossing and boundary objects. Review of educational research, 81(2), pp.132-169. <a href="https://doi.org/10.3102/0034654311404435">DOI:10.3102/0034654311404435</a>.</p> </li> <li> <p>S. Pauletto and Y. Seznec (2024). Connecting Sound to Data: Sonification Workshop Methods With Expert and Non-Expert Participants, J. Audio Eng. Soc. Vol. 72, np. 5, pp. 328-340. <a href="https://doi.org/10.17743/jaes.2022.0143">DOI:10.17743/jaes.2022.0143</a>.</p> </li> <li> <p>Drever, J.L. (2019). Primacy of the Ear” – But Whose Ear?: The case for auraldiversity in sonic arts practice and discourse, Organised Sound, 24(1), pp. 85–95. <a href="https://doi.org/10.1017/S1355771819000086">doi:10.1017/S1355771819000086</a>.</p> </li> <li> <p>Seiça, M., Roque, L., Martins, P. et al. (2023) An interdisciplinary journey towards an aesthetics of sonification experience. J Multimodal User Interfaces 17, 263–284. <a href="https://doi.org/10.1007/s12193-023-00416-7">DOI:10.1007/s12193-023-00416-7</a></p> </li> <li> <p>Wirfs-Brock, J., Perera, J. and Geere, D., 2024, July. Open sonifications: a manifesto for many ecologies of data and sound. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (pp. 2660-2674). <a href="https://dl.acm.org/doi/10.1145/3643834.3660757">DOI:10.1145/3643834.3660757</a></p> </li> <li> <p>Supper, A. (2012). Lobbying for the ear : the public fascination with and academic legitimacy of the sonification of scientific data. [Doctoral Thesis, Maastricht University]. Datawyse / Universitaire Pers Maastricht. <a href="https://doi.org/10.26481/dis.20120606as">DOI:10.26481/dis.20120606as</a> pages 92-101 (section 3.3)</p> </li> <li> <p>Archer MO, et al. (2022) Listening to the Magnetosphere: How Best to Make ULF Waves Audible. Front. Astron. Space Sci. 9:877172. <a href="https://doi.org/10.3389/fspas.2022.877172">DOI:10.3389/fspas.2022.877172</a></p> </li> </ol>]]></content><author><name></name></author><category term="paper-of-the-fortnight"/><summary type="html"><![CDATA[Each fortnight a member of the Sonic Intangibles team select a paper to read and discuss. Our reading list is below:]]></summary></entry><entry><title type="html">Direct Segmented Sonification</title><link href="https://sonicintangibles.github.io/stories/2024/DSSon/" rel="alternate" type="text/html" title="Direct Segmented Sonification"/><published>2024-07-23T14:12:00+00:00</published><updated>2024-07-23T14:12:00+00:00</updated><id>https://sonicintangibles.github.io/stories/2024/DSSon</id><content type="html" xml:base="https://sonicintangibles.github.io/stories/2024/DSSon/"><![CDATA[<p>Paul Vickers and Robert Höldrich presented <em>Direct Segmented Sonification</em>, or DSSon <a class="citation" href="#Vickers:2019b">(Vickers &amp; Höldrich, 2019)</a> as a way of sonifying one-dimensional time series data in a temporally compressed form that picks out key features in the data.</p> <p>Data was gathered from <a href="https://paulvickers.github.io/SoniFRED/">users on the FRED machine</a>. Below are two examples of using DSSon.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/DSSon_ADV_B_n.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/DSSon_ADV_A_e.mp3" controls=""/> </figure> </div> </div> <div class="caption"> The first example is a DSSOn of one minute of a first-time user's data. The second one is one minute of data taken from an expert user. </div> <p>The DSSon here emphasised <em>poor technique</em>, so the less sound you hear, the better the user is operating the machine.</p>]]></content><author><name></name></author><category term="previous-work"/><category term="audio"/><summary type="html"><![CDATA[Sonifying one-dimensional time-series data]]></summary></entry></feed>