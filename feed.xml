<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sonicintangibles.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sonicintangibles.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T13:53:00+00:00</updated><id>https://sonicintangibles.github.io/feed.xml</id><title type="html">Sonic Intangibles</title><subtitle>A website for the Sonic Intangibles project. </subtitle><entry><title type="html">Paper of The Fortnight</title><link href="https://sonicintangibles.github.io/log/2025/POTF/" rel="alternate" type="text/html" title="Paper of The Fortnight"/><published>2025-05-11T00:00:00+00:00</published><updated>2025-05-11T00:00:00+00:00</updated><id>https://sonicintangibles.github.io/log/2025/POTF</id><content type="html" xml:base="https://sonicintangibles.github.io/log/2025/POTF/"><![CDATA[<p>Each fortnight a member of the Sonic Intangibles team select a paper to read and discuss. Our reading list is below:</p> <ol> <li> <p>Akkerman, S.F. and Bakker, A., 2011. Boundary crossing and boundary objects. Review of educational research, 81(2), pp.132-169. <a href="https://doi.org/10.3102/0034654311404435">DOI:10.3102/0034654311404435</a>.</p> </li> <li> <p>S. Pauletto and Y. Seznec (2024). Connecting Sound to Data: Sonification Workshop Methods With Expert and Non-Expert Participants, J. Audio Eng. Soc. Vol. 72, np. 5, pp. 328-340. <a href="https://doi.org/10.17743/jaes.2022.0143">DOI:10.17743/jaes.2022.0143</a>.</p> </li> <li> <p>Drever, J.L. (2019). Primacy of the Ear” – But Whose Ear?: The case for auraldiversity in sonic arts practice and discourse, Organised Sound, 24(1), pp. 85–95. <a href="https://doi.org/10.1017/S1355771819000086">doi:10.1017/S1355771819000086</a>.</p> </li> <li> <p>Seiça, M., Roque, L., Martins, P. et al. (2023) An interdisciplinary journey towards an aesthetics of sonification experience. J Multimodal User Interfaces 17, 263–284. <a href="https://doi.org/10.1007/s12193-023-00416-7">DOI:10.1007/s12193-023-00416-7</a></p> </li> <li> <p>Wirfs-Brock, J., Perera, J. and Geere, D., 2024, July. Open sonifications: a manifesto for many ecologies of data and sound. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (pp. 2660-2674). <a href="https://dl.acm.org/doi/10.1145/3643834.3660757">DOI:10.1145/3643834.3660757</a></p> </li> <li> <p>Supper, A. (2012). Lobbying for the ear : the public fascination with and academic legitimacy of the sonification of scientific data. [Doctoral Thesis, Maastricht University]. Datawyse / Universitaire Pers Maastricht. <a href="https://doi.org/10.26481/dis.20120606as">DOI:10.26481/dis.20120606as</a> pages 92-101 (section 3.3)</p> </li> <li> <p>Archer MO, et al. (2022) Listening to the Magnetosphere: How Best to Make ULF Waves Audible. Front. Astron. Space Sci. 9:877172. <a href="https://doi.org/10.3389/fspas.2022.877172">DOI:10.3389/fspas.2022.877172</a></p> </li> </ol>]]></content><author><name></name></author><category term="paper-of-the-fortnight"/><summary type="html"><![CDATA[Each fortnight a member of the Sonic Intangibles team select a paper to read and discuss. Our reading list is below:]]></summary></entry><entry><title type="html">Project Start</title><link href="https://sonicintangibles.github.io/log/2024/start/" rel="alternate" type="text/html" title="Project Start"/><published>2024-12-05T00:00:00+00:00</published><updated>2024-12-05T00:00:00+00:00</updated><id>https://sonicintangibles.github.io/log/2024/start</id><content type="html" xml:base="https://sonicintangibles.github.io/log/2024/start/"><![CDATA[<p>The project will officially begin on 1 February, 2025. Howver, be sure to check back before then for news of preliminary activities and calls for participation that we will be putting out in the New Year.</p>]]></content><author><name>Paul Vickers</name></author><summary type="html"><![CDATA[The project will officially begin on 1 February, 2025. Howver, be sure to check back before then for news of preliminary activities and calls for participation that we will be putting out in the New Year.]]></summary></entry><entry><title type="html">Direct Segmented Sonification</title><link href="https://sonicintangibles.github.io/log/2024/DSSon/" rel="alternate" type="text/html" title="Direct Segmented Sonification"/><published>2024-07-23T14:12:00+00:00</published><updated>2024-07-23T14:12:00+00:00</updated><id>https://sonicintangibles.github.io/log/2024/DSSon</id><content type="html" xml:base="https://sonicintangibles.github.io/log/2024/DSSon/"><![CDATA[<p>Paul Vickers and Robert Höldrich presented <em>Direct Segmented Sonification</em>, or DSSon <a class="citation" href="#Vickers:2019b">(Vickers &amp; Höldrich, 2019)</a> as a way of sonifying one-dimensional time series data in a temporally compressed form that picks out key features in the data.</p> <p>Data was gathered from <a href="https://paulvickers.github.io/SoniFRED/">users on the FRED machine</a>. Below are two examples of using DSSon.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/DSSon_ADV_B_n.mp3" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/DSSon_ADV_A_e.mp3" controls=""/> </figure> </div> </div> <div class="caption"> The first example is a DSSOn of one minute of a first-time user's data. The second one is one minute of data taken from an expert user. </div> <p>The DSSon here emphasised <em>poor technique</em>, so the less sound you hear, the better the user is operating the machine.</p>]]></content><author><name></name></author><category term="previous-work"/><category term="audio"/><summary type="html"><![CDATA[Sonifying one-dimensional time-series data]]></summary></entry><entry><title type="html">We’re hiring post-doctoral researchers!</title><link href="https://sonicintangibles.github.io/log/2024/hiring/" rel="alternate" type="text/html" title="We’re hiring post-doctoral researchers!"/><published>2024-07-19T00:00:00+00:00</published><updated>2024-07-19T00:00:00+00:00</updated><id>https://sonicintangibles.github.io/log/2024/hiring</id><content type="html" xml:base="https://sonicintangibles.github.io/log/2024/hiring/"><![CDATA[<h2 id="recruitment-closed">RECRUITMENT CLOSED</h2> <p><strong>These positions have now been filled. Thanks to all who applied.</strong></p> <p>The project needs two post-doctoral researchers to play a leading role in the project activities. One post will be based at Northumbria University and the other at neighbouring Newcastle University. Please follow the links below to see each of the job advertisements:</p> <p>We are seeking two researchers, one at Northumbria University and one at neighbouring Newcastle University, to play a central role in this exciting interdisciplinary project that brings together researchers from a range of fields (Music, Computer Science, Astronomy, Mathematics, Space Weather, and Materials Science) to work on developing sonification solutions as a truly interdisciplinary activity.</p> <p>Northumbria University and Newcastle University are both located in Newcastle-upon-Tyne in north-east England, UK. The project team is based at both universities and are within 5–10 minutes’ walk of each other.</p> <p>The project is expected to commence in January 2025. Click on the links below to apply for either position:</p> <ul> <li><a href="https://work4.northumbria.ac.uk/#en/sites/CX_1001/job/2601">Northumbria University: Research Fellow in Computer Science (Sonic Intangibles)</a> — If you would like an informal discussion about the role, please contact the Project Lead, Professor Paul Vickers at paul.vickers@northumbria.ac.uk</li> <li><a href="https://jobs.ncl.ac.uk/job/Newcastle-Research-Associate-Sonic-Intangibles/1095690301/">Newcastle University Research Associate: Sonic Intangibles</a> — For an informal discussion about this post please contact Dr Bennett Hogg at bennett.hogg@ncl.ac.uk.</li> </ul> <p>To apply you will need to include a <strong>covering letter</strong> and a <strong>CV</strong> that includes <strong>a full list of your publications/outputs</strong>.</p> <p>The closing date for applications for both posts is <strong>6 October</strong> and interviews are expected to take place in the <strong>week beginning 21 October</strong>.</p>]]></content><author><name>Paul Vickers</name></author><summary type="html"><![CDATA[RECRUITMENT CLOSED These positions have now been filled. Thanks to all who applied.]]></summary></entry></feed>